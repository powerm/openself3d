{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
      "10000 10000 10000\n",
      "[[190.344 229.599 188.467 ... 158.95  232.165 285.74 ]\n",
      " [254.616 220.92  243.608 ... 247.35  243.183 215.875]\n",
      " [  0.824   0.789   0.823 ...   0.85    0.787   0.785]]\n",
      "(3, 10000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import mmcv\n",
    "from PIL  import Image\n",
    " \n",
    "img_a_mask= Image.open('/Users/johnny/Code/openself3d/data/2018-04-16-14-25-19/processed/image_masks/000008_mask.png')\n",
    "img_a_mask = np.asarray(img_a_mask)\n",
    "img_a_depth= Image.open('/Users/johnny/Code/openself3d/data/2018-04-16-14-25-19/processed/images/000008_depth.png')\n",
    "img_a_depth = np.asarray(img_a_depth)\n",
    "\n",
    "mask_indices = np.where(img_a_mask>0,1,0)\n",
    "depth_indices = np.where(img_a_depth>0,1,0)\n",
    "select_indices = mask_indices&depth_indices\n",
    "img_a_indices = np.where(select_indices>0)\n",
    "rand_indices = np.random.randint(0,len(img_a_indices[0]),[10000])\n",
    "u_vec = img_a_indices[0][rand_indices]\n",
    "v_vec = img_a_indices[1][rand_indices]\n",
    "uv_vec = (u_vec, v_vec)\n",
    "img_a_depth_float = img_a_depth.astype(np.float)\n",
    "DEPTH_IM_SCALE = 1000.0 #将厘米转换为米。\n",
    "depth_vec_float = img_a_depth_float[uv_vec]*1.0/DEPTH_IM_SCALE\n",
    "u_vec_float = u_vec.astype(np.float)*depth_vec_float\n",
    "v_vec_float = v_vec.astype(np.float)*depth_vec_float\n",
    "print(u_vec_float.size,v_vec_float.size,depth_vec_float.size)\n",
    "#print(u_vec_float,v_vec_float, depth_vec_float)\n",
    "\n",
    "z_vec_float = depth_vec_float\n",
    "full_vec_float = np.vstack((u_vec_float,v_vec_float, z_vec_float))\n",
    "print(full_vec_float)\n",
    "print(full_vec_float.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "body_to_rdf = np.zeros((3,3))\n",
    "body_to_rdf[0, 1] = -1.0\n",
    "body_to_rdf[1, 2] = -1.0\n",
    "body_to_rdf[2, 0] = 1.0\n",
    "\n",
    "rdf_to_body = np.linalg.inv(body_to_rdf)\n",
    "K = np.array([[1,0,0],\n",
    "              [0,1,0],\n",
    "              [0,0,1]\n",
    "              ])\n",
    "K[0,0] = 533.6422696034836 # focal x\n",
    "K[1,1] = 534.7824445233571 # focal y\n",
    "K[0,2] = 319.4091030774892 # principal point x\n",
    "K[1,2] = 236.4374299691866 # principal point y\n",
    "K[2,2] = 1.0\n",
    "K_inv = np.linalg.inv(K)\n",
    "point_camera_frame_rdf_vec = np.matmul(K_inv,full_vec_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import mmcv \n",
    "from correspondence_finder import apply_transform, invert_transform\n",
    "\n",
    "scene_pose_data = mmcv.load('/Users/johnny/Code/openself3d/data/2018-04-16-14-25-19/processed/images/pose_data.yaml')\n",
    "a_pose_data = scene_pose_data[8]['camera_to_world']\n",
    "img_a_pose = utils.homogenous_transform_from_dict(a_pose_data)\n",
    "b_pose_data = scene_pose_data[163]['camera_to_world']\n",
    "img_b_pose = utils.homogenous_transform_from_dict(b_pose_data)\n",
    "\n",
    "point_world_frame_rdf_vec = apply_transform(point_camera_frame_rdf_vec, img_a_pose)\n",
    "point_camera_2_frame_rdf_vec = apply_transform(point_world_frame_rdf_vec,invert_transform(img_b_pose))\n",
    "\n",
    "########### vec2#########\n",
    "vec2_vec = np.matmul(K, point_camera_2_frame_rdf_vec)\n",
    "u2_vec = vec2_vec[0]/vec2_vec[2]\n",
    "v2_vec = vec2_vec[1]/vec2_vec[2]\n",
    "\n",
    "maybe_z2_vec = point_camera_2_frame_rdf_vec[2]\n",
    "z2_vec = vec2_vec[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "# Prune based on \n",
    "# Case 2: the pixels projected into image b are outside FOV\n",
    "# u2_vec bounds should be: 0 image_width\n",
    "# v2_vec bounds should be: 0, image_height\n",
    "## do u2-based pruning\n",
    "\n",
    "image_width = 640\n",
    "image_height = 480\n",
    "\n",
    "u2_vec_lower_bound = 0.0\n",
    "epsilon = 1e-3\n",
    "u2_vec_upper_bound = image_height*1.0-epsilon\n",
    "\n",
    "v2_vec_lower_bound = 0.0\n",
    "v2_vec_upper_bound = image_width*1.0-epsilon\n",
    "\n",
    "#lower_bound_vec = np.ones_like(u2_vec)* u2_vec_lower_bound\n",
    "#upper_bound_vec = np.ones_like(u2_vec)* u2_vec_upper_bound\n",
    "#zeros_vec = np.ones_like(u2_vec)\n",
    "\n",
    "u2_bound_indices = np.where((u2_vec>u2_vec_lower_bound)&(u2_vec<u2_vec_upper_bound),1,0)\n",
    "v2_bound_indices = np.where((v2_vec>v2_vec_lower_bound)&(v2_vec<v2_vec_upper_bound),1,0)\n",
    "in_bound_indices = u2_bound_indices&v2_bound_indices\n",
    "in_indices = np.where(in_bound_indices>0)\n",
    "\n",
    "u2_vec_prune = u2_vec[in_indices]\n",
    "v2_vec_prune = v2_vec[in_indices]\n",
    "z2_vec_prune = z2_vec[in_indices]\n",
    "#u_vec_prune = u_vec[in_indices]\n",
    "#v_vec_prune = v_vec[in_indices]\n",
    "#z_vec_prune = z_vec_float[in_indices]\n",
    "\n",
    "# Prune based on \n",
    "# Case 3: the pixels in image b are occluded, OR there is no depth return in image b so we aren't sure\n",
    "\n",
    "\n",
    "u2_vec_long = u2_vec_prune.astype(np.int32)\n",
    "v2_vec_long = v2_vec_prune.astype(np.int32)\n",
    "uv2_vec_long = (u2_vec_long, v2_vec_long)\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "img_b_depth = Image.open('/Users/johnny/Code/openself3d/data/2018-04-16-14-25-19/processed/images/000163_depth.png')\n",
    "img_b_depth = np.asarray(img_b_depth)\n",
    "\n",
    "img_b_depth_float = img_b_depth.astype(np.float)\n",
    "depth2_vec = img_b_depth_float[uv2_vec_long]*1.0/DEPTH_IM_SCALE\n",
    "\n",
    "occlusion_margin = 0.003\n",
    "z2_vec_prune = z2_vec_prune - occlusion_margin\n",
    "\n",
    "depth2_01 = np.where((depth2_vec>0)&(depth2_vec>z2_vec_prune),1,0)\n",
    "non_occluded_indices = np.where(depth2_01>0)\n",
    "depth2_vec_prune = depth2_vec[non_occluded_indices]\n",
    "\n",
    "u2_vec_long_prune = u2_vec_long[non_occluded_indices]\n",
    "v2_vec_long_prune = v2_vec_long[non_occluded_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1,0,0],[0,1,0],[0,0,1],[0,0,0]])\n",
    "b = np.ones_like(a[0,:]).reshape(1,-1)\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "c = np.hstack((a,b))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import correspondence_finder\n",
    "import correspondence_plotter\n",
    "import time \n",
    "from spartan import SceneStructure\n",
    "\n",
    "K = np.array([[1,0,0],\n",
    "              [0,1,0],\n",
    "              [0,0,1]\n",
    "              ])\n",
    "K[0,0] = 533.6422696034836 # focal x\n",
    "K[1,1] = 534.7824445233571 # focal y\n",
    "K[0,2] = 319.4091030774892 # principal point x\n",
    "K[1,2] = 236.4374299691866 # principal point y\n",
    "K[2,2] = 1.0\n",
    "\n",
    "\n",
    "scenes_dir =\"/Users/johnny/Code/openself3d/data\"\n",
    "sceneStruct= SceneStructure(scenes_dir)\n",
    "scene = \"2018-04-16-14-25-19\"\n",
    "img_a_idx = sceneStruct.get_random_image_index(scene)\n",
    "img_a_rgb, img_a_depth, img_a_mask, img_a_pose = sceneStruct.get_rgbd_mask_pose(scene, img_a_idx)\n",
    "\n",
    "img_b_idx = sceneStruct.get_img_idx_with_different_pose(scene, img_a_pose, num_attempts=50)\n",
    "img_b_rgb, img_b_depth, img_b_mask, img_b_pose = sceneStruct.get_rgbd_mask_pose(scene, img_b_idx)\n",
    "\n",
    "img_a_depth_numpy = np.asarray(img_a_depth)\n",
    "img_b_depth_numpy = np.asarray(img_b_depth)\n",
    "\n",
    "start = time.time()\n",
    "uv_a = (300, 200)\n",
    "uv_a, uv_b = correspondence_finder.batch_find_pixel_correspondences(img_a_depth_numpy, img_a_pose,\n",
    "                                                                    img_b_depth_numpy, img_b_pose,\n",
    "                                                                \n",
    "                                                                    )\n",
    "print(time.time()-start, \"seconds\")\n",
    "if uv_a is not None:\n",
    "    correspondence_plotter.plot_correspondences_direct(img_a_rgb, img_a_depth_numpy, img_b_rgb, img_b_depth_numpy, uv_a,uv_b)\n",
    "else:\n",
    "    print(\"try running this cell again, did not find a correspondence for this pixel\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32c33e10c18490d0f0253f8d5484f96fa64472ed5162ac3fe4579bac37362991"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
